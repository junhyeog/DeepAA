{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JVP and VJP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fn(x) = (4,) \n",
      "[9.61084  7.59375  8.542969]\n",
      "\n",
      "## JVP ##\n",
      "full_jacobian = (3, 4) \n",
      "[[ 57.66504   76.88672   57.66504    9.61084 ]\n",
      " [ 15.1875    45.5625    25.3125    11.390625]\n",
      " [ 42.714844 119.60156   39.867188  25.628906]]\n",
      "\n",
      "v = (4,) \n",
      "[0.2 0.3 0.4 0.8]\n",
      "\n",
      "jvp_ = (3,) \n",
      "[65.353714 35.943752 80.87344 ]\n",
      "\n",
      "f_evaluated = (3,) \n",
      "[9.61084  7.59375  8.542969]\n",
      "\n",
      "jvp_evaluated = (3,) \n",
      "[65.353714 35.943752 80.87344 ]\n",
      "\n",
      "## VJP ##\n",
      "full_jacobian = (3, 4) \n",
      "[[ 57.66504   76.88672   57.66504    9.61084 ]\n",
      " [ 15.1875    45.5625    25.3125    11.390625]\n",
      " [ 42.714844 119.60156   39.867188  25.628906]]\n",
      "\n",
      "v = (3,) \n",
      "[0.5 0.8 1. ]\n",
      "\n",
      "vjp_ = (4,) \n",
      "[ 83.697365 194.49492   88.94971   39.546825]\n",
      "\n",
      "f_evaluated = (3,) \n",
      "[9.61084  7.59375  8.542969]\n",
      "\n",
      "vjp = (4,) \n",
      "[ 83.697365 194.49492   88.94971   39.546825]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "\n",
    "def fn(x):\n",
    "    u = jnp.array(\n",
    "        [\n",
    "            x[0] ** 6 * x[1] ** 4 * x[2] ** 9 * x[3] ** 2,\n",
    "            x[0] ** 2 * x[1] ** 3 * x[2] ** 5 * x[3] ** 3,\n",
    "            x[0] ** 5 * x[1] ** 7 * x[2] ** 7 * x[3] ** 6,\n",
    "        ]\n",
    "    )\n",
    "    return u\n",
    "\n",
    "\n",
    "x = jnp.array([1.0, 0.5, 1.5, 2.0])\n",
    "print(f\"fn(x) = {x.shape} \\n{fn(x)}\\n\")\n",
    "\n",
    "##############################################################\n",
    "print(\"## JVP ##\")\n",
    "\n",
    "# jacovian: df/dx\n",
    "full_jacobian = jax.jacfwd(fn)(x)\n",
    "print(f\"full_jacobian = {full_jacobian.shape} \\n{full_jacobian}\\n\")\n",
    "\n",
    "# JVP: df/dx @ v\n",
    "## 1.\n",
    "v = jnp.array([0.2, 0.3, 0.4, 0.8])\n",
    "print(f\"v = {v.shape} \\n{v}\\n\")\n",
    "jvp_ = full_jacobian @ v\n",
    "print(f\"jvp_ = {jvp_.shape} \\n{jvp_}\\n\")\n",
    "\n",
    "## 2.\n",
    "f_evaluated, jvp_evaluated = jax.jvp(fn, (x,), (v,))\n",
    "print(f\"f_evaluated = {f_evaluated.shape} \\n{f_evaluated}\\n\")\n",
    "print(f\"jvp_evaluated = {jvp_evaluated.shape} \\n{jvp_evaluated}\\n\")\n",
    "\n",
    "##############################################################\n",
    "print(\"## VJP ##\")\n",
    "\n",
    "# jacovian: df/dx\n",
    "full_jacobian = jax.jacrev(fn)(x)\n",
    "print(f\"full_jacobian = {full_jacobian.shape} \\n{full_jacobian}\\n\")\n",
    "\n",
    "# VJP: df/dx @ v\n",
    "## 1.\n",
    "v = jnp.array([0.5, 0.8, 1.0])\n",
    "print(f\"v = {v.shape} \\n{v}\\n\")\n",
    "vjp_ = v.T @ full_jacobian\n",
    "print(f\"vjp_ = {vjp_.shape} \\n{vjp_}\\n\")\n",
    "\n",
    "## 2.\n",
    "f_evaluated, vjp_fn = jax.vjp(fn, x)\n",
    "print(f\"f_evaluated = {f_evaluated.shape} \\n{f_evaluated}\\n\")\n",
    "vjp = vjp_fn(v)[0]\n",
    "print(f\"vjp = {vjp.shape} \\n{vjp}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## JVP ##\n",
      "func_output = torch.Size([3]) \n",
      "tensor([9.6108, 7.5938, 8.5430])\n",
      "\n",
      "jvp = torch.Size([3]) \n",
      "tensor([65.3537, 35.9438, 80.8734])\n",
      "\n",
      "## JVP with torch.func.jvp ##\n",
      "func_output = torch.Size([3]) \n",
      "tensor([9.6108, 7.5938, 8.5430], grad_fn=<AliasBackward0>)\n",
      "\n",
      "jvp = torch.Size([3]) \n",
      "tensor([65.3537, 35.9438, 80.8734], grad_fn=<StackBackward0>)\n",
      "\n",
      "## VJP ##\n",
      "func_output = torch.Size([3]) \n",
      "tensor([9.6108, 7.5938, 8.5430])\n",
      "\n",
      "vjp = torch.Size([4]) \n",
      "tensor([ 83.6974, 194.4949,  88.9497,  39.5468])\n",
      "\n",
      "## VJP with torch.func.vjp ##\n",
      "func_output = torch.Size([3]) \n",
      "tensor([9.6108, 7.5938, 8.5430], grad_fn=<StackBackward0>)\n",
      "\n",
      "vjp = torch.Size([4]) \n",
      "tensor([ 83.6974, 194.4949,  88.9497,  39.5468], grad_fn=<AddBackward0>)\n",
      "\n",
      "## VJP with torch.autograd.grad ##\n",
      "y = torch.Size([3]) \n",
      "tensor([9.6108, 7.5938, 8.5430], grad_fn=<StackBackward0>)\n",
      "\n",
      "grad = torch.Size([4]) \n",
      "tensor([ 83.6974, 194.4949,  88.9497,  39.5468])\n",
      "\n",
      "func_output = torch.Size([3]) \n",
      "tensor([9.6108, 7.5938, 8.5430], grad_fn=<StackBackward0>)\n",
      "\n",
      "vjp = torch.Size([4]) \n",
      "tensor([ 83.6974, 194.4949,  88.9497,  39.5468], grad_fn=<AddBackward0>)\n",
      "\n",
      "## J ##\n",
      "j = torch.Size([3, 4]) \n",
      "tensor([[ 57.6650,  76.8867,  57.6650,   9.6108],\n",
      "        [ 15.1875,  45.5625,  25.3125,  11.3906],\n",
      "        [ 42.7148, 119.6016,  39.8672,  25.6289]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "jvp = torch.Size([3]) \n",
      "tensor([65.3537, 35.9437, 80.8734], grad_fn=<MvBackward0>)\n",
      "\n",
      "j = torch.Size([3, 4]) \n",
      "tensor([[ 57.6650,  76.8867,  57.6650,   9.6108],\n",
      "        [ 15.1875,  45.5625,  25.3125,  11.3906],\n",
      "        [ 42.7148, 119.6016,  39.8672,  25.6289]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "v = torch.Size([3]) \n",
      "tensor([0.5000, 0.8000, 1.0000])\n",
      "\n",
      "vjp = torch.Size([4]) \n",
      "tensor([ 83.6974, 194.4949,  88.9497,  39.5468], grad_fn=<SqueezeBackward4>)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1230/1573156851.py:74: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3571.)\n",
      "  vjp = v.T @ j\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def fn(x):\n",
    "    return torch.stack(\n",
    "        [\n",
    "            x[0] ** 6 * x[1] ** 4 * x[2] ** 9 * x[3] ** 2,\n",
    "            x[0] ** 2 * x[1] ** 3 * x[2] ** 5 * x[3] ** 3,\n",
    "            x[0] ** 5 * x[1] ** 7 * x[2] ** 7 * x[3] ** 6,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "x = torch.tensor([1.0, 0.5, 1.5, 2.0], requires_grad=True)\n",
    "##############################################################\n",
    "print(\"## JVP ##\")\n",
    "v = torch.tensor([0.2, 0.3, 0.4, 0.8])\n",
    "func_output, jvp = torch.autograd.functional.jvp(fn, x, v)\n",
    "print(f\"func_output = {func_output.shape} \\n{func_output}\\n\")\n",
    "print(f\"jvp = {jvp.shape} \\n{jvp}\\n\")\n",
    "\n",
    "\n",
    "##############################################################\n",
    "print(\"## JVP with torch.func.jvp ##\")\n",
    "v = torch.tensor([0.2, 0.3, 0.4, 0.8])\n",
    "func_output, jvp = torch.func.jvp(fn, (x,), (v,))\n",
    "print(f\"func_output = {func_output.shape} \\n{func_output}\\n\")\n",
    "print(f\"jvp = {jvp.shape} \\n{jvp}\\n\")\n",
    "\n",
    "\n",
    "##############################################################\n",
    "print(\"## VJP ##\")\n",
    "v = torch.tensor([0.5, 0.8, 1.0])\n",
    "func_output, vjp = torch.autograd.functional.vjp(fn, x, v)\n",
    "print(f\"func_output = {func_output.shape} \\n{func_output}\\n\")\n",
    "print(f\"vjp = {vjp.shape} \\n{vjp}\\n\")\n",
    "\n",
    "##############################################################\n",
    "print(\"## VJP with torch.func.vjp ##\")\n",
    "v = torch.tensor([0.5, 0.8, 1.0])\n",
    "func_output, vjp_fn = torch.func.vjp(fn, x)\n",
    "vjp = vjp_fn(v)[0]\n",
    "print(f\"func_output = {func_output.shape} \\n{func_output}\\n\")\n",
    "print(f\"vjp = {vjp.shape} \\n{vjp}\\n\")\n",
    "\n",
    "##############################################################\n",
    "print(\"## VJP with torch.autograd.grad ##\")\n",
    "v = torch.tensor([0.5, 0.8, 1.0])\n",
    "y = fn(x)\n",
    "print(f\"y = {y.shape} \\n{y}\\n\")\n",
    "grad = torch.autograd.grad(y, x, grad_outputs=v)[0]  # == vjp\n",
    "print(f\"grad = {grad.shape} \\n{grad}\\n\")\n",
    "\n",
    "\n",
    "print(f\"func_output = {func_output.shape} \\n{func_output}\\n\")\n",
    "print(f\"vjp = {vjp.shape} \\n{vjp}\\n\")\n",
    "\n",
    "\n",
    "# ##############################################################\n",
    "print(\"## J ##\")\n",
    "\n",
    "v = torch.tensor([0.2, 0.3, 0.4, 0.8])\n",
    "j_rev_fn = torch.func.jacrev(fn)\n",
    "j = j_rev_fn(x)\n",
    "print(f\"j = {j.shape} \\n{j}\\n\")\n",
    "jvp = j @ v\n",
    "print(f\"jvp = {jvp.shape} \\n{jvp}\\n\")\n",
    "\n",
    "j_fwd_fn = torch.func.jacfwd(fn)\n",
    "j = j_fwd_fn(x)\n",
    "print(f\"j = {j.shape} \\n{j}\\n\")\n",
    "v = torch.tensor([0.5, 0.8, 1.0])\n",
    "print(f\"v = {v.shape} \\n{v}\\n\")\n",
    "vjp = v.T @ j\n",
    "print(f\"vjp = {vjp.shape} \\n{vjp}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-18 19:35:50.004039: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## VJP ##\n",
      "y = (3,) \n",
      "[9.61084  7.59375  8.542969]\n",
      "\n",
      "vjp = (4,) \n",
      "[ 83.697365 194.49492   88.94971   39.546825]\n",
      "\n",
      "## JVP ##\n",
      "y = (3,) \n",
      "[9.61084  7.59375  8.542969]\n",
      "\n",
      "jvp = (3,) \n",
      "[65.353714 35.943752 80.87344 ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def fn(x):\n",
    "    return tf.stack(\n",
    "        [\n",
    "            x[0] ** 6 * x[1] ** 4 * x[2] ** 9 * x[3] ** 2,\n",
    "            x[0] ** 2 * x[1] ** 3 * x[2] ** 5 * x[3] ** 3,\n",
    "            x[0] ** 5 * x[1] ** 7 * x[2] ** 7 * x[3] ** 6,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "x = tf.Variable([1.0, 0.5, 1.5, 2.0], dtype=tf.float32)\n",
    "\n",
    "##############################################################\n",
    "print(\"## VJP ##\")\n",
    "v = tf.constant([0.5, 0.8, 1.0], dtype=tf.float32)\n",
    "with tf.GradientTape() as tape:\n",
    "    y = fn(x)\n",
    "vjp = tape.gradient(y, x, output_gradients=v)\n",
    "print(f\"y = {y.shape} \\n{y}\\n\")\n",
    "print(f\"vjp = {vjp.shape} \\n{vjp}\\n\")\n",
    "\n",
    "##############################################################\n",
    "print(\"## JVP ##\")\n",
    "v = tf.constant([0.2, 0.3, 0.4, 0.8], dtype=tf.float32)\n",
    "with tf.autodiff.ForwardAccumulator(primals=x, tangents=v) as acc:\n",
    "    y = fn(x)\n",
    "jvp = acc.jvp(y)\n",
    "print(f\"y = {y.shape} \\n{y}\\n\")\n",
    "print(f\"jvp = {jvp.shape} \\n{jvp}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(-13.69874, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# with model\n",
    "x = tf.constant([[2.0, 3.0], [1.0, 4.0]])\n",
    "targets = tf.constant([[1.0], [-1.0]])\n",
    "dense = tf.keras.layers.Dense(1)\n",
    "dense.build([None, 2])\n",
    "with tf.autodiff.ForwardAccumulator(primals=dense.kernel, tangents=tf.constant([[1.0], [0.0]])) as acc:\n",
    "    loss = tf.reduce_sum((dense(x) - targets) ** 2.0)\n",
    "print(acc.jvp(loss))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JVP and VJP with model input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(-5.216034, shape=(), dtype=float32)\n",
      "dense.kernel\n",
      "<tf.Variable 'kernel:0' shape=(2, 1) dtype=float32, numpy=\n",
      "array([[ 1.2166749],\n",
      "       [-0.7691392]], dtype=float32)>\n",
      "\n",
      "tf.constant([[1.], [0.]])\n",
      "tf.Tensor(\n",
      "[[1.]\n",
      " [0.]], shape=(2, 1), dtype=float32)\n",
      "\n",
      "tf.Tensor(\n",
      "[[2. 3.]\n",
      " [1. 4.]], shape=(2, 2), dtype=float32)\n",
      "tf.Tensor(1.3227375, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant([[2.0, 3.0], [1.0, 4.0]])\n",
    "targets = tf.constant([[1.0], [-1.0]])\n",
    "dense = tf.keras.layers.Dense(1)\n",
    "dense.build([None, 2])\n",
    "with tf.autodiff.ForwardAccumulator(primals=dense.kernel, tangents=tf.constant([[1.0], [0.0]])) as acc:\n",
    "    loss = tf.reduce_sum((dense(x) - targets) ** 2.0)\n",
    "print(acc.jvp(loss))\n",
    "\n",
    "\n",
    "print(\"dense.kernel\")\n",
    "print(dense.kernel)\n",
    "print()\n",
    "print(\"tf.constant([[1.], [0.]])\")\n",
    "print(tf.constant([[1.0], [0.0]]))\n",
    "print()\n",
    "print(x)\n",
    "tangents = tf.constant([[0.0, 0], [0.0, 1.0]])\n",
    "with tf.autodiff.ForwardAccumulator(primals=x, tangents=tangents) as acc:\n",
    "    loss = tf.reduce_sum((dense(x) - targets) ** 2.0)\n",
    "print(acc.jvp(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2995, 0.2864, 0.5609], grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size, feature_size = 3, 5\n",
    "weights = torch.randn(feature_size, requires_grad=True)\n",
    "\n",
    "\n",
    "def model(feature_vec):\n",
    "    # Very simple linear model with activation\n",
    "    return feature_vec.dot(weights).relu()\n",
    "\n",
    "\n",
    "examples = torch.randn(batch_size, feature_size)\n",
    "result = torch.vmap(model)(examples)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def run_JVP(imgs, labs):\n",
    "    L = tf.shape(imgs)[0] # T\n",
    "    grad_importance_array = tf.TensorArray(\n",
    "        tf.float32, size=0, dynamic_size=True, infer_shape=False, element_shape=[None]\n",
    "    )\n",
    "    grad_importance_array, _ = tf.while_loop(\n",
    "        cond=lambda grad_TA, k: tf.cast(k, dtype=tf.int32)\n",
    "        < tf.cast(tf.math.ceil(tf.cast(L, dtype=tf.float32) / tf.cast(bs, dtype=tf.float32)), dtype=tf.int32),\n",
    "        # ! cond = k < ceil(T/_PARALLEL_BATCH)\n",
    "        body=lambda grad_TA, k: (\n",
    "            one_step_JVP(\n",
    "                grad_TA,\n",
    "                imgs[batching(L, bs, k)[0] : batching(L, bs, k)[1]],  # ! [_PARALLEL_BATCH, *img_size]\n",
    "                labs[batching(L, bs, k)[0] : batching(L, bs, k)[1]],  # ! [_PARALLEL_BATCH]\n",
    "                k,\n",
    "            ),\n",
    "            k + 1,\n",
    "        ),\n",
    "        loop_vars=(grad_importance_array, tf.constant(0)),\n",
    "        back_prop=False,\n",
    "        parallel_iterations=1,\n",
    "    )\n",
    "    return grad_importance_array.concat()  # ! [T, ??]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: shape=(3, 2), dtype=float32, numpy=\n",
      "array([[0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.]], dtype=float32)>, <tf.Tensor: shape=(3, 2), dtype=float32, numpy=\n",
      "array([[0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.]], dtype=float32)>]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Incompatible shape for value ((2, 3, 2)), expected ((None,))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[39mreturn\u001b[39;00m aa\n\u001b[1;32m     13\u001b[0m aa \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mTensorArray(tf\u001b[39m.\u001b[39mfloat32, size\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, dynamic_size\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, infer_shape\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, element_shape\u001b[39m=\u001b[39m[\u001b[39mNone\u001b[39;00m])\n\u001b[0;32m---> 14\u001b[0m aa, _ \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mwhile_loop(\n\u001b[1;32m     15\u001b[0m     cond\u001b[39m=\u001b[39;49m\u001b[39mlambda\u001b[39;49;00m grad_TA, k: tf\u001b[39m.\u001b[39;49mcast(k, dtype\u001b[39m=\u001b[39;49mtf\u001b[39m.\u001b[39;49mint32) \u001b[39m<\u001b[39;49m tf\u001b[39m.\u001b[39;49mcast(\u001b[39m5\u001b[39;49m, dtype\u001b[39m=\u001b[39;49mtf\u001b[39m.\u001b[39;49mint32),\n\u001b[1;32m     16\u001b[0m     body\u001b[39m=\u001b[39;49m\u001b[39mlambda\u001b[39;49;00m grad_TA, k: (\n\u001b[1;32m     17\u001b[0m         fn(grad_TA, k),\n\u001b[1;32m     18\u001b[0m         k \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m,\n\u001b[1;32m     19\u001b[0m     ),\n\u001b[1;32m     20\u001b[0m     loop_vars\u001b[39m=\u001b[39;49m(aa, tf\u001b[39m.\u001b[39;49mconstant(\u001b[39m0\u001b[39;49m)),\n\u001b[1;32m     21\u001b[0m     back_prop\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     22\u001b[0m     parallel_iterations\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m     23\u001b[0m )\n\u001b[1;32m     25\u001b[0m aa\u001b[39m.\u001b[39mconcat()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/util/deprecation.py:648\u001b[0m, in \u001b[0;36mdeprecated_arg_values.<locals>.deprecated_wrapper.<locals>.new_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    640\u001b[0m           _PRINTED_WARNING[(func, arg_name)] \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    641\u001b[0m         logging\u001b[39m.\u001b[39mwarning(\n\u001b[1;32m    642\u001b[0m             \u001b[39m'\u001b[39m\u001b[39mFrom \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m: calling \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m (from \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m) with \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m=\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m is deprecated and \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    643\u001b[0m             \u001b[39m'\u001b[39m\u001b[39mwill be removed \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mInstructions for updating:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    646\u001b[0m             \u001b[39m'\u001b[39m\u001b[39min a future version\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m date \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m\n\u001b[1;32m    647\u001b[0m             (\u001b[39m'\u001b[39m\u001b[39mafter \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m date), instructions)\n\u001b[0;32m--> 648\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/ops/control_flow_ops.py:2574\u001b[0m, in \u001b[0;36mwhile_loop_v2\u001b[0;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, maximum_iterations, name)\u001b[0m\n\u001b[1;32m   2368\u001b[0m \u001b[39m@tf_export\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mwhile_loop\u001b[39m\u001b[39m\"\u001b[39m, v1\u001b[39m=\u001b[39m[])\n\u001b[1;32m   2369\u001b[0m \u001b[39m@deprecation\u001b[39m\u001b[39m.\u001b[39mdeprecated_arg_values(\n\u001b[1;32m   2370\u001b[0m     \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2385\u001b[0m                   maximum_iterations\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   2386\u001b[0m                   name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m   2387\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Repeat `body` while the condition `cond` is true.\u001b[39;00m\n\u001b[1;32m   2388\u001b[0m \n\u001b[1;32m   2389\u001b[0m \u001b[39m  Note: This op is automatically used in a `tf.function` to convert Python for-\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2572\u001b[0m \n\u001b[1;32m   2573\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2574\u001b[0m   \u001b[39mreturn\u001b[39;00m while_loop(\n\u001b[1;32m   2575\u001b[0m       cond\u001b[39m=\u001b[39;49mcond,\n\u001b[1;32m   2576\u001b[0m       body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m   2577\u001b[0m       loop_vars\u001b[39m=\u001b[39;49mloop_vars,\n\u001b[1;32m   2578\u001b[0m       shape_invariants\u001b[39m=\u001b[39;49mshape_invariants,\n\u001b[1;32m   2579\u001b[0m       parallel_iterations\u001b[39m=\u001b[39;49mparallel_iterations,\n\u001b[1;32m   2580\u001b[0m       back_prop\u001b[39m=\u001b[39;49mback_prop,\n\u001b[1;32m   2581\u001b[0m       swap_memory\u001b[39m=\u001b[39;49mswap_memory,\n\u001b[1;32m   2582\u001b[0m       name\u001b[39m=\u001b[39;49mname,\n\u001b[1;32m   2583\u001b[0m       maximum_iterations\u001b[39m=\u001b[39;49mmaximum_iterations,\n\u001b[1;32m   2584\u001b[0m       return_same_structure\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/ops/control_flow_ops.py:2823\u001b[0m, in \u001b[0;36mwhile_loop\u001b[0;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name, maximum_iterations, return_same_structure)\u001b[0m\n\u001b[1;32m   2820\u001b[0m loop_var_structure \u001b[39m=\u001b[39m nest\u001b[39m.\u001b[39mmap_structure(type_spec\u001b[39m.\u001b[39mtype_spec_from_value,\n\u001b[1;32m   2821\u001b[0m                                         \u001b[39mlist\u001b[39m(loop_vars))\n\u001b[1;32m   2822\u001b[0m \u001b[39mwhile\u001b[39;00m cond(\u001b[39m*\u001b[39mloop_vars):\n\u001b[0;32m-> 2823\u001b[0m   loop_vars \u001b[39m=\u001b[39m body(\u001b[39m*\u001b[39;49mloop_vars)\n\u001b[1;32m   2824\u001b[0m   \u001b[39mif\u001b[39;00m try_to_pack \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(loop_vars, (\u001b[39mlist\u001b[39m, _basetuple)):\n\u001b[1;32m   2825\u001b[0m     packed \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[17], line 17\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(grad_TA, k)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[39mreturn\u001b[39;00m aa\n\u001b[1;32m     13\u001b[0m aa \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mTensorArray(tf\u001b[39m.\u001b[39mfloat32, size\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, dynamic_size\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, infer_shape\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, element_shape\u001b[39m=\u001b[39m[\u001b[39mNone\u001b[39;00m])\n\u001b[1;32m     14\u001b[0m aa, _ \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mwhile_loop(\n\u001b[1;32m     15\u001b[0m     cond\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m grad_TA, k: tf\u001b[39m.\u001b[39mcast(k, dtype\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mint32) \u001b[39m<\u001b[39m tf\u001b[39m.\u001b[39mcast(\u001b[39m5\u001b[39m, dtype\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mint32),\n\u001b[1;32m     16\u001b[0m     body\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m grad_TA, k: (\n\u001b[0;32m---> 17\u001b[0m         fn(grad_TA, k),\n\u001b[1;32m     18\u001b[0m         k \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m,\n\u001b[1;32m     19\u001b[0m     ),\n\u001b[1;32m     20\u001b[0m     loop_vars\u001b[39m=\u001b[39m(aa, tf\u001b[39m.\u001b[39mconstant(\u001b[39m0\u001b[39m)),\n\u001b[1;32m     21\u001b[0m     back_prop\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m     22\u001b[0m     parallel_iterations\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m     23\u001b[0m )\n\u001b[1;32m     25\u001b[0m aa\u001b[39m.\u001b[39mconcat()\n",
      "Cell \u001b[0;32mIn[17], line 10\u001b[0m, in \u001b[0;36mfn\u001b[0;34m(aa, k)\u001b[0m\n\u001b[1;32m      7\u001b[0m tmp \u001b[39m=\u001b[39m [i \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m tmp]\n\u001b[1;32m      9\u001b[0m \u001b[39mprint\u001b[39m(tmp)\n\u001b[0;32m---> 10\u001b[0m aa \u001b[39m=\u001b[39m aa\u001b[39m.\u001b[39;49mwrite(k, tmp)\n\u001b[1;32m     11\u001b[0m \u001b[39mreturn\u001b[39;00m aa\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/util/tf_should_use.py:243\u001b[0m, in \u001b[0;36mshould_use_result.<locals>.decorated.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 243\u001b[0m   \u001b[39mreturn\u001b[39;00m _add_should_use_warning(fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs),\n\u001b[1;32m    244\u001b[0m                                  warn_in_eager\u001b[39m=\u001b[39mwarn_in_eager,\n\u001b[1;32m    245\u001b[0m                                  error_in_function\u001b[39m=\u001b[39merror_in_function)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/ops/tensor_array_ops.py:1176\u001b[0m, in \u001b[0;36mTensorArray.write\u001b[0;34m(self, index, value, name)\u001b[0m\n\u001b[1;32m   1160\u001b[0m \u001b[39m@tf_should_use\u001b[39m\u001b[39m.\u001b[39mshould_use_result(warn_in_eager\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m   1161\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrite\u001b[39m(\u001b[39mself\u001b[39m, index, value, name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m   1162\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Write `value` into index `index` of the TensorArray.\u001b[39;00m\n\u001b[1;32m   1163\u001b[0m \n\u001b[1;32m   1164\u001b[0m \u001b[39m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1174\u001b[0m \u001b[39m    ValueError: if there are more writers than specified.\u001b[39;00m\n\u001b[1;32m   1175\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1176\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_implementation\u001b[39m.\u001b[39;49mwrite(index, value, name\u001b[39m=\u001b[39;49mname)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/ops/tensor_array_ops.py:846\u001b[0m, in \u001b[0;36m_EagerTensorArray.write\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    844\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"See TensorArray.\"\"\"\u001b[39;00m\n\u001b[1;32m    845\u001b[0m \u001b[39mdel\u001b[39;00m name  \u001b[39m# not meaningful when executing eagerly.\u001b[39;00m\n\u001b[0;32m--> 846\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_write(index, value)\n\u001b[1;32m    847\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparent()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/ops/tensor_array_ops.py:835\u001b[0m, in \u001b[0;36m_EagerTensorArray._write\u001b[0;34m(self, index, value)\u001b[0m\n\u001b[1;32m    829\u001b[0m   \u001b[39mraise\u001b[39;00m errors_impl\u001b[39m.\u001b[39mInvalidArgumentError(\n\u001b[1;32m    830\u001b[0m       \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    831\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mTensorArray dtype is \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m but Op is trying to write dtype \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m    832\u001b[0m       (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dtype\u001b[39m.\u001b[39mname, value\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mname))\n\u001b[1;32m    834\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_element_shape\u001b[39m.\u001b[39mis_compatible_with(value\u001b[39m.\u001b[39mshape):\n\u001b[0;32m--> 835\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mIncompatible shape for value (\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m), expected (\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m    836\u001b[0m                    (value\u001b[39m.\u001b[39mshape, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_element_shape))\n\u001b[1;32m    838\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_infer_shape:\n\u001b[1;32m    839\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_element_shape \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_element_shape\u001b[39m.\u001b[39mmerge_with(value\u001b[39m.\u001b[39mshape)\n",
      "\u001b[0;31mValueError\u001b[0m: Incompatible shape for value ((2, 3, 2)), expected ((None,))"
     ]
    }
   ],
   "source": [
    "# tf.TensorArray Test\n",
    "import tensorflow as tf\n",
    "\n",
    "def fn(aa, k):\n",
    "    tmp =tf.ones(12, dtype=tf.float32) * tf.cast(k, dtype=tf.float32)\n",
    "    tmp = tf.reshape(tmp, [2, 3, 2])\n",
    "    tmp = [i for i in tmp]\n",
    "    \n",
    "    print(tmp)\n",
    "    aa = aa.write(k, tmp)\n",
    "    return aa\n",
    "\n",
    "aa = tf.TensorArray(tf.float32, size=0, dynamic_size=True, infer_shape=False, element_shape=[None])\n",
    "aa, _ = tf.while_loop(\n",
    "    cond=lambda grad_TA, k: tf.cast(k, dtype=tf.int32) < tf.cast(5, dtype=tf.int32),\n",
    "    body=lambda grad_TA, k: (\n",
    "        fn(grad_TA, k),\n",
    "        k + 1,\n",
    "    ),\n",
    "    loop_vars=(aa, tf.constant(0)),\n",
    "    back_prop=False,\n",
    "    parallel_iterations=1,\n",
    ")\n",
    "\n",
    "aa.concat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(50,), dtype=float32, numpy=\n",
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 3., 3., 3., 3.,\n",
       "       3., 3., 3., 3., 3., 3., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
