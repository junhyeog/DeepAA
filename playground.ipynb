{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JVP and VJP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fn(x) = (4,) \n",
      "[9.61084  7.59375  8.542969]\n",
      "\n",
      "## JVP ##\n",
      "full_jacobian = (3, 4) \n",
      "[[ 57.66504   76.88672   57.66504    9.61084 ]\n",
      " [ 15.1875    45.5625    25.3125    11.390625]\n",
      " [ 42.714844 119.60156   39.867188  25.628906]]\n",
      "\n",
      "v = (4,) \n",
      "[0.2 0.3 0.4 0.8]\n",
      "\n",
      "jvp_ = (3,) \n",
      "[65.353714 35.943752 80.87344 ]\n",
      "\n",
      "f_evaluated = (3,) \n",
      "[9.61084  7.59375  8.542969]\n",
      "\n",
      "jvp_evaluated = (3,) \n",
      "[65.353714 35.943752 80.87344 ]\n",
      "\n",
      "## VJP ##\n",
      "full_jacobian = (3, 4) \n",
      "[[ 57.66504   76.88672   57.66504    9.61084 ]\n",
      " [ 15.1875    45.5625    25.3125    11.390625]\n",
      " [ 42.714844 119.60156   39.867188  25.628906]]\n",
      "\n",
      "v = (3,) \n",
      "[0.5 0.8 1. ]\n",
      "\n",
      "vjp_ = (4,) \n",
      "[ 83.697365 194.49492   88.94971   39.546825]\n",
      "\n",
      "f_evaluated = (3,) \n",
      "[9.61084  7.59375  8.542969]\n",
      "\n",
      "vjp = (4,) \n",
      "[ 83.697365 194.49492   88.94971   39.546825]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "def fn(x):\n",
    "    u=jnp.array([\n",
    "        x[0]**6 * x[1]**4 * x[2]**9 * x[3]**2,\n",
    "        x[0]**2 * x[1]**3 * x[2]**5 * x[3]**3,\n",
    "        x[0]**5 * x[1]**7 * x[2]**7 * x[3]**6,\n",
    "    ])\n",
    "    return u\n",
    "\n",
    "x = jnp.array([1.0, 0.5, 1.5, 2.0])\n",
    "print(f\"fn(x) = {x.shape} \\n{fn(x)}\\n\")\n",
    "\n",
    "##############################################################\n",
    "print(\"## JVP ##\")\n",
    "\n",
    "# jacovian: df/dx\n",
    "full_jacobian = jax.jacfwd(fn)(x)\n",
    "print(f\"full_jacobian = {full_jacobian.shape} \\n{full_jacobian}\\n\")\n",
    "\n",
    "# JVP: df/dx @ v\n",
    "## 1.\n",
    "v = jnp.array([0.2, 0.3, 0.4, 0.8])\n",
    "print(f\"v = {v.shape} \\n{v}\\n\")\n",
    "jvp_ = full_jacobian @ v\n",
    "print(f\"jvp_ = {jvp_.shape} \\n{jvp_}\\n\")\n",
    "\n",
    "## 2.\n",
    "f_evaluated, jvp_evaluated = jax.jvp(fn, (x,), (v,))\n",
    "print(f\"f_evaluated = {f_evaluated.shape} \\n{f_evaluated}\\n\")\n",
    "print(f\"jvp_evaluated = {jvp_evaluated.shape} \\n{jvp_evaluated}\\n\")\n",
    "\n",
    "##############################################################\n",
    "print(\"## VJP ##\")\n",
    "\n",
    "# jacovian: df/dx\n",
    "full_jacobian = jax.jacrev(fn)(x)\n",
    "print(f\"full_jacobian = {full_jacobian.shape} \\n{full_jacobian}\\n\")\n",
    "\n",
    "# VJP: df/dx @ v\n",
    "## 1.\n",
    "v = jnp.array([0.5, 0.8, 1.0])\n",
    "print(f\"v = {v.shape} \\n{v}\\n\")\n",
    "vjp_ = v.T @ full_jacobian\n",
    "print(f\"vjp_ = {vjp_.shape} \\n{vjp_}\\n\")\n",
    "\n",
    "## 2.\n",
    "f_evaluated, vjp_fn = jax.vjp(fn, x)\n",
    "print(f\"f_evaluated = {f_evaluated.shape} \\n{f_evaluated}\\n\")\n",
    "vjp = vjp_fn(v)[0]\n",
    "print(f\"vjp = {vjp.shape} \\n{vjp}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## JVP ##\n",
      "func_output = torch.Size([3]) \n",
      "tensor([9.6108, 7.5938, 8.5430])\n",
      "\n",
      "jvp = torch.Size([3]) \n",
      "tensor([65.3537, 35.9438, 80.8734])\n",
      "\n",
      "## VJP ##\n",
      "func_output = torch.Size([3]) \n",
      "tensor([9.6108, 7.5938, 8.5430])\n",
      "\n",
      "vjp = torch.Size([4]) \n",
      "tensor([ 83.6974, 194.4949,  88.9497,  39.5468])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def fn(x):\n",
    "    return torch.stack([\n",
    "        x[0]**6 * x[1]**4 * x[2]**9 * x[3]**2,\n",
    "        x[0]**2 * x[1]**3 * x[2]**5 * x[3]**3,\n",
    "        x[0]**5 * x[1]**7 * x[2]**7 * x[3]**6,\n",
    "    ])\n",
    "\n",
    "x = torch.tensor([1.0, 0.5, 1.5, 2.0], requires_grad=True)\n",
    "##############################################################\n",
    "print(\"## JVP ##\")\n",
    "v = torch.tensor([0.2, 0.3, 0.4, 0.8])\n",
    "func_output, jvp = torch.autograd.functional.jvp(fn, x, v)\n",
    "print(f\"func_output = {func_output.shape} \\n{func_output}\\n\")\n",
    "print(f\"jvp = {jvp.shape} \\n{jvp}\\n\")\n",
    "\n",
    "##############################################################\n",
    "print(\"## VJP ##\")\n",
    "v = torch.tensor([0.5, 0.8, 1.0])\n",
    "func_output, vjp = torch.autograd.functional.vjp(fn, x, v)\n",
    "print(f\"func_output = {func_output.shape} \\n{func_output}\\n\")\n",
    "print(f\"vjp = {vjp.shape} \\n{vjp}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-16 13:39:14.326055: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## JVP ##\n",
      "y = (3,) \n",
      "[9.61084  7.59375  8.542969]\n",
      "\n",
      "jvp = (3,) \n",
      "[65.353714 35.943752 80.87344 ]\n",
      "\n",
      "## VJP ##\n",
      "y = (3,) \n",
      "[9.61084  7.59375  8.542969]\n",
      "\n",
      "vjp = (4,) \n",
      "[ 83.697365 194.49492   88.94971   39.546825]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def fn(x):\n",
    "    return tf.stack([\n",
    "        x[0]**6 * x[1]**4 * x[2]**9 * x[3]**2,\n",
    "        x[0]**2 * x[1]**3 * x[2]**5 * x[3]**3,\n",
    "        x[0]**5 * x[1]**7 * x[2]**7 * x[3]**6,\n",
    "    ])\n",
    "\n",
    "x = tf.Variable([1.0, 0.5, 1.5, 2.0], dtype=tf.float32)\n",
    "##############################################################\n",
    "print(\"## JVP ##\")\n",
    "v = tf.constant([0.2, 0.3, 0.4, 0.8], dtype=tf.float32)\n",
    "with tf.autodiff.ForwardAccumulator(primals=x, tangents=v) as acc:\n",
    "    y= fn(x)\n",
    "jvp = acc.jvp(y)\n",
    "print(f\"y = {y.shape} \\n{y}\\n\")\n",
    "print(f\"jvp = {jvp.shape} \\n{jvp}\\n\")\n",
    "\n",
    "##############################################################\n",
    "print(\"## VJP ##\")\n",
    "v = tf.constant([0.5, 0.8, 1.0], dtype=tf.float32)\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(x)\n",
    "    y = fn(x)\n",
    "vjp = tape.gradient(y, x, output_gradients=v)\n",
    "print(f\"y = {y.shape} \\n{y}\\n\")\n",
    "print(f\"vjp = {vjp.shape} \\n{vjp}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(-1.7002447, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant([[2.0, 3.0], [1.0, 4.0]])\n",
    "targets = tf.constant([[1.], [-1.]])\n",
    "dense = tf.keras.layers.Dense(1)\n",
    "dense.build([None, 2])\n",
    "with tf.autodiff.ForwardAccumulator(\n",
    "   primals=dense.kernel,\n",
    "   tangents=tf.constant([[1.], [0.]])) as acc:\n",
    "  loss = tf.reduce_sum((dense(x) - targets) ** 2.)\n",
    "print(acc.jvp(loss))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JVP and VJP with model input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(11.96414, shape=(), dtype=float32)\n",
      "dense.kernel\n",
      "<tf.Variable 'kernel:0' shape=(2, 1) dtype=float32, numpy=\n",
      "array([[-1.3399825],\n",
      "       [ 1.3681983]], dtype=float32)>\n",
      "\n",
      "tf.constant([[1.], [0.]])\n",
      "tf.Tensor(\n",
      "[[1.]\n",
      " [0.]], shape=(2, 1), dtype=float32)\n",
      "\n",
      "tf.Tensor(\n",
      "[[2. 3.]\n",
      " [1. 4.]], shape=(2, 2), dtype=float32)\n",
      "tf.Tensor(14.045405, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant([[2.0, 3.0], [1.0, 4.0]])\n",
    "targets = tf.constant([[1.], [-1.]])\n",
    "dense = tf.keras.layers.Dense(1)\n",
    "dense.build([None, 2])\n",
    "with tf.autodiff.ForwardAccumulator(\n",
    "   primals=dense.kernel,\n",
    "   tangents=tf.constant([[1.], [0.]])) as acc:\n",
    "  loss = tf.reduce_sum((dense(x) - targets) ** 2.)\n",
    "print(acc.jvp(loss))\n",
    "\n",
    "\n",
    "print(\"dense.kernel\")\n",
    "print(dense.kernel)\n",
    "print()\n",
    "print(\"tf.constant([[1.], [0.]])\")\n",
    "print(tf.constant([[1.], [0.]]))\n",
    "print()\n",
    "print(x)\n",
    "tangents = tf.constant([[0., 0], [0., 1.]])\n",
    "with tf.autodiff.ForwardAccumulator(\n",
    "   primals=x,\n",
    "   tangents=tangents\n",
    "  ) as acc:\n",
    "  loss = tf.reduce_sum((dense(x) - targets) ** 2.)\n",
    "print(acc.jvp(loss))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
